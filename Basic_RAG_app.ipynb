{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "aPu772PWiOgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline\n",
        "import PyPDF2\n",
        "import textwrap\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import logging\n",
        "import os\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZaghcRBiV6h",
        "outputId": "8fa15d40-d8eb-45f5-cfef-e53acb8b2a60"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Logging"
      ],
      "metadata": {
        "id": "qBwj08FLianQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")"
      ],
      "metadata": {
        "id": "rawCLdDZib5w"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Text Processing Utilities"
      ],
      "metadata": {
        "id": "VdSNB0kmiep4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            text = ''\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text() + '\\n'\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to extract text from {pdf_path}: {str(e)}\")\n",
        "        return ''\n",
        "\n",
        "def preprocess_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentences = text.split('.')\n",
        "    cleaned_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.lower().split()\n",
        "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "        cleaned_sentences.append(\" \".join(words))\n",
        "    return cleaned_sentences\n"
      ],
      "metadata": {
        "id": "9DQQvezeihRo"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the RAG Class"
      ],
      "metadata": {
        "id": "PMF-NL0kioNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAG:\n",
        "    def __init__(self, documents, top_k=2, max_length=100):\n",
        "        self.documents = documents\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.doc_vectors = self.vectorizer.fit_transform(documents)\n",
        "        self.generator = pipeline('text-generation', model='gpt2')\n",
        "        self.summarizer = pipeline('summarization', model='facebook/bart-large-cnn')\n",
        "        self.top_k = top_k\n",
        "        self.max_length = max_length\n",
        "        self.cache = {}\n",
        "\n",
        "    def retrieve_documents(self, query):\n",
        "        if query in self.cache:\n",
        "            logging.info(f\"Retrieving results from cache for query: {query}\")\n",
        "            return self.cache[query]\n",
        "\n",
        "        query_vector = self.vectorizer.transform([query])\n",
        "        similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()\n",
        "        top_indices = similarities.argsort()[-self.top_k:][::-1]\n",
        "        retrieved_docs = [self.documents[i] for i in top_indices]\n",
        "        scores = similarities[top_indices]\n",
        "\n",
        "        self.cache[query] = (retrieved_docs, scores)\n",
        "        return retrieved_docs, scores\n",
        "\n",
        "    def generate_answer(self, query):\n",
        "        relevant_docs, scores = self.retrieve_documents(query)\n",
        "        logging.info(\"Retrieved documents:\")\n",
        "        for i, (doc, score) in enumerate(zip(relevant_docs, scores)):\n",
        "            logging.info(f\"Doc {i+1} - Score: {score:.4f} - Text: {doc[:100]}...\")\n",
        "\n",
        "        context = \" \".join(relevant_docs)\n",
        "        summarized_context = self.summarizer(context, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
        "\n",
        "        logging.info(f\"Summarized context: {summarized_context}\")\n",
        "\n",
        "        prompt = f\"Context: {summarized_context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "        response = self.generator(prompt, max_length=self.max_length, num_return_sequences=1)\n",
        "\n",
        "        return response[0]['generated_text']\n"
      ],
      "metadata": {
        "id": "8gbpg4GCiq1x"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function for PDF Processing and User Interaction"
      ],
      "metadata": {
        "id": "LtSrbG3liuhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    pdf_paths = [\n",
        "        'data/data.pdf',  # Update this to your PDF file paths\n",
        "    ]\n",
        "\n",
        "    all_documents = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        if not os.path.exists(pdf_path):\n",
        "            logging.error(f\"File not found: {pdf_path}\")\n",
        "            continue\n",
        "        logging.info(f\"Extracting text from {pdf_path}...\")\n",
        "        pdf_text = extract_text_from_pdf(pdf_path)\n",
        "        if pdf_text:\n",
        "            logging.info(f\"Preprocessing text from {pdf_path}...\")\n",
        "            documents = preprocess_text(pdf_text)\n",
        "            all_documents.extend(documents)\n",
        "\n",
        "    if not all_documents:\n",
        "        logging.error(\"No valid documents extracted from the PDFs. Exiting.\")\n",
        "        return\n",
        "\n",
        "    logging.info(\"Initializing RAG model...\")\n",
        "    rag = RAG(all_documents)\n",
        "\n",
        "    print(\"\\nRAG model is ready. You can now ask questions about the PDF content.\")\n",
        "    print(\"Type 'quit' to exit the application.\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your question: \")\n",
        "        if query.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        answer = rag.generate_answer(query)\n",
        "        print(\"\\nAnswer:\")\n",
        "        wrapped_answer = textwrap.fill(answer, width=80)\n",
        "        print(wrapped_answer)\n",
        "        print()\n",
        "\n",
        "    print(\"Thank you for using the RAG CLI Application!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuFLCyXcjDUI",
        "outputId": "71eeff22-37b4-4e47-ebcc-97146c6b6c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RAG model is ready. You can now ask questions about the PDF content.\n",
            "Type 'quit' to exit the application.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}